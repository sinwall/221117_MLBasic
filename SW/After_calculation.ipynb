{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "output_path = './output/'\n",
    "try:\n",
    "    _=os.listdir(output_path)\n",
    "except:\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation modules\n",
    "from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay,RocCurveDisplay\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score, accuracy_score,roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from data_loader_default import load_data_default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_score(y_train,y_pred):\n",
    "    accuracy = accuracy_score(y_train,y_pred)\n",
    "    print(f'accuracy : {accuracy}')\n",
    "    pre_score = precision_score(y_train,y_pred)\n",
    "    print(f'precision : {pre_score}')\n",
    "    rec_score = recall_score(y_train,y_pred)\n",
    "    print(f'recall : {rec_score}')\n",
    "    f_score = f1_score(y_train,y_pred)\n",
    "    print(f'f1_score: {f_score}')\n",
    "\n",
    "    return [round(accuracy,4), round(pre_score,4), round(rec_score ,4), round(f_score,4)]\n",
    "\n",
    "\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds,ax=None):\n",
    "    if ax is None:\n",
    "        plt.plot(thresholds, precisions[:-1], 'b--', label=\"Precision\")\n",
    "        plt.plot(thresholds, recalls[:-1], 'g-',label=\"recall\")\n",
    "        plt.xlabel(\"thresholds\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.ylim([0,1])\n",
    "    else:\n",
    "        ax.plot(thresholds, precisions[:-1], 'b--', label=\"Precision\")\n",
    "        ax.plot(thresholds, recalls[:-1], 'g-',label=\"recall\")\n",
    "        ax.set_xlabel(\"thresholds\")\n",
    "        ax.legend(loc=\"upper left\")\n",
    "        ax.set_ylim([0,1])\n",
    "\n",
    "\n",
    "X_train,y_train,X_val,y_val = 0,0,0,0\n",
    "#def plot_my_graphs(pred_train,pred_val,score_val,name=None):\n",
    "\n",
    "score_dataframes = []\n",
    "for num,(X_train ,y_train, X_val, y_val) in enumerate(load_data.yield_data()):\n",
    "    pred_train = pd.read_csv('filepath'+'dd.csv')\n",
    "    pred_val = pd.read_csv('filepath'+'dd.csv')\n",
    "    score_val = pd.read_csv('filepath'+'dd.csv')\n",
    "    scores =pd.Series(my_score(y_val,pred_val)+[round(roc_auc_score(y_val,score_val),4)]\n",
    "                        ,index=['acc','prec','rec','f1','roc_auc'])\n",
    "    score_dataframes.append(scores)\n",
    "\n",
    "df = pd.concat(score_dataframes, axis=0)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_py_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9abb538871b4e03b427e9482af4cbea7329408220ff7b8d0c3bc51f0a54e037b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
